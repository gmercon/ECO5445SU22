---
title: "Project Stage 2"
author: "Guillermo"
date: '2022-08-01'
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: 3
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(readr)
library(pROC)
library(gamlr)
library(boot)
library(randomForest)
library(e1071)
```

In this final project, you will use R in order to evaluate logistic regression models with the objective of building a mortgage application approval/denial classifier. Of interest is modeling the probability that an applicant will be approved or denied as a function observables in order to classify applicants as being ones who will be approved and who will not be approved. The dataset and first questions are the same as Stage 1, but now you will need to use R.

From the GitHub repository, download the .csv file. The dataset contains records on 2,380 applicants for mortgages in Boston. Also at our repo is a description of the variables and a paper published in the American Economic Review that first evaluated the data in order to test for the presence of racial discrimination in the mortgage approval process. You should first read the paper as it provides a detailed discussion of the data and insight on the determinants of the probability of being approved. After reading the paper and importing the dataset into R, do the following:

# Part A

A. The variables in the dataset do not have intuitive names (e.g., the meaning of S3 is unclear). Referencing the data description and the AER paper, identify the qualitative dependent that you will be modeling and the set of covariates that you intend to include in your various models, and rename the variables so that they have (somewhat) intuitive names. Be certain that the debt-to-income ratio and the race, self-employed, marital status, and education indicator variables are included, among other variables.

## Bringing in the Data and renaming columns

```{r}
Mortgagestats <- read_csv("Data/hmda_sw.csv",show_col_types = FALSE)

Mortgagestats <- Mortgagestats[c(5,6,9,11,13,20,24,30,36,39,40,59)]

# s6, s7, s13, s15, s17, s23a, s27a, s33,s42, s45, S46, school

Coln = c("Loan_Amount", "Action_Taken", "Race","Sex", "Income_Thousands", "Marital_Status", "Self_Employed", "Purchase_Price", "Mortgage_Credit_History", "DTI_Housing", "DTI_Total", "School")

colnames(Mortgagestats) <- Coln
```

## Examining Data

- Here we bring in the data and reduce it to the variables that we hypothesize will affect the chances of getting a mortgage application approved. 

Action Taken: This is the dependent variable, the action taken can either be (1) for approved or (0) for denied

Loan Amount: The amount on the application for the loan. This variable divided by the purchase price can give us a ratio that shows what percentage of the purchase the loan will cover. The rest of the purchase may be covered with a down payment, so a low ratio shows that the loan is only a small portion of the purchase price. I hypothesize that as this ratio goes up the likelihood to get approved will go down

Race: This data is separated into white as (1), and black as (0). I will hypothesize that if you are white it will be easier to get a mortgage approved than if you are black. This will mean the data will lean towards (1).

Sex: Males are coded as (1) and females as (0). A few NA's were changed to 1 for simplicity and to not have to remove the data point entirely. I hypothesize if you are a man it is more likely for you to get approved.

Income in Thousands: This shows the yearly income of the applicant in thousands. I hypothesize that the higher your income is the easier it will be for you to get approved significantly. .

Marital Status: Married is shown as (M), separated as (S), and Unmarried(single,divorced,widow) as (U). I hypothesize that if you are married it will increase your chances of getting approved. Unmarried and separated may negatively affect your chances but most likely will affect it very little as there are many different outcomes to divorce and separation that leave one party one well and the other in shambles. 

Self-employed: If designated self employment status they are a (1), if not then (0). I hypothesize that self employment may improve your chances of getting an application. People who are self employed and applying for mortgages may be in a period of high income which would boost their chances of approval. 

Purchase_Price: The purchase price in thousands of dollars. This data interacting with the loan amount may give us higher insight. Refer to loan amount above.

Mortgage_Credit_History: This variables shows us the applicants previous credit history specifically relating to mortgages. it starts at (1) with no late mortgage payments, (2) shows no mortgage history, (3) is for one or two late payments, and (4) is more than two late payments. I hypothesize that as your credit history increases it will negatively impact your chances of getting approved. 

DTI_Housing: Housing Debt to income ratio. If this ratio is (1) then your housing debt is 100% of your total income. I hypothesize that as this ratio increases it will negatively affect your chances to get approvedd.  

DTI_Total:Total Debt to income ratio. Similar to housing, if this ratio is (1) then your total debt is 100% of your total income. I hypothesize that as this ratio increases it will cause the chances of getting approved to down. 

School: Years of education. At (12) years applicants have finished high school and (16) or above signifies that they finished a degree in higher education. I hypothesize that as their years of education go up it will increase the chances of getting a mortgage. 


## Cleaning and factoring the data

```{r}
Mortgagestats[Mortgagestats == 999999.4] <- NA
Mortgagestats <- na.omit(Mortgagestats)

Mortgagestats$Race <- factor(Mortgagestats$Race, levels = c(3,5),labels = c(0,1))
Mortgagestats$Action_Taken <- factor(Mortgagestats$Action_Taken, levels = c(1,2,3), labels = c(1,1,0))
Mortgagestats$Sex <- factor(Mortgagestats$Sex, levels = c(1,2,3), labels = c(1,0,1))
Mortgagestats$Marital_Status <- factor(Mortgagestats$Marital_Status)
Mortgagestats$Mortgage_Credit_History <- factor(Mortgagestats$Mortgage_Credit_History)

Mortgagestats$DTI_Housing <- Mortgagestats$DTI_Housing/100
Mortgagestats$DTI_Total <- Mortgagestats$DTI_Total/100
```

- As we brought in the data we reduced the number of columns to variables deemed important, also 
renamed the columns so they represent the data point.
- From there we change any missing data points that were previously coded as 999,999.4 to NA. We then remove any NA values throughout the data because this will cause problems modeling later on. 
- All categorical data is factored so they can be counted and assessed properly. Otherwise for example, in race black is represented by 3 and white by 5, our data would think that means black gets 3 units while white gets 5, skewing the points. Factoring fixes those issues. 
- The DTI ratios are reduced to decimal places so that if you have 1 DTI then your debts equal 100% of your income. 


# Part B

B. Generate summary statistics on the set of variables selected in A, and explain the composition of the sample and of the characteristics of an average (representative) applicant. In the process, you should also generate and histograms and frequency counts on particular variables of interest, which can be referenced in your explanation of the composition of the sample and of a representative applicant.

## Statistics and Standard Deviation

```{r}
summary(Mortgagestats)
sd(Mortgagestats$Loan_Amount, na.rm = TRUE)
sd(Mortgagestats$Income_Thousands, na.rm =TRUE)
sd(Mortgagestats$Purchase_Price, na.rm = TRUE)
sd(Mortgagestats$DTI_Housing, na.rm = TRUE)
sd(Mortgagestats$DTI_Total, na.rm = TRUE)
sd(Mortgagestats$School, na.rm = TRUE)
```

## Analyzing correlation between variables

```{r}
cordata <-Mortgagestats

cordata$Action_Taken <- as.numeric(levels(cordata$Action_Taken))[cordata$Action_Taken]
cordata$Race <- as.numeric(levels(cordata$Race))[cordata$Race]
cordata$Sex <- as.numeric(levels(cordata$Sex))[cordata$Sex]
cordata$Marital_Status <- as.numeric(cordata$Marital_Status)
cordata$Mortgage_Credit_History <- as.numeric(levels(cordata$Mortgage_Credit_History))[cordata$Mortgage_Credit_History]

cor(cordata[-c(2)],cordata[c(2)])
```
- having factors in the data can cause issues when checking for correlation. We can fix this by changing the factors to their values, but this may not always work when some factors have multiple levels. Either way we can examine some of the coefficients
- We see that race increases your chance of approval positively by about .21%. Your sex will have a very small effect on your chances of approval.Although income has a slightly more tangible affect than sex, it is still very small as it will positively affect you by .004%. Your marital status negatively affects your chances of getting approved by 8%, in this case as your marital status increases you are moving towards unmarried, which fits our hypothesis. Being self employed will negatively affect your chances by about 4%. The purchase price will positively affect your chances, to me this indicates that people applying for more expensive homes are getting approved more often than not. As your mortgage credit history worsens it will decrease your chances of approval by 11% which matches our hypothesis. Housing DTI and Total DTI will decrease you chances of getting approval as your DTI ratios increase. This makes perfect sense as higher levels of debt cause banks to stop giving you loans. As we hypothesized, your years of education will positively impact your chances of getting a loan by about 8%. 


## Histograms

```{r}
hist(Mortgagestats$Loan_Amount, breaks = "fd", xlab = "Loan Amount(Thousands)")
hist(Mortgagestats$Income_Thousands, breaks = "fd", xlab = "Income(Thousands)")
hist(Mortgagestats$Purchase_Price, breaks = "fd", xlab = "Purchase Price(Thousands)")
hist(Mortgagestats$DTI_Housing, breaks = "fd", xlab = "Debt to Income, Housing Debt")
hist(Mortgagestats$DTI_Total, breaks = "fd", xlab = "Debt to Income, Total Debt")
hist(Mortgagestats$School, breaks = "fd",xlab = "Years of Education")
hist((Mortgagestats$Loan_Amount/Mortgagestats$Purchase_Price), breaks = "fd", xlab = "Loan to Purchase Price Ratio", main = "Histogram of Loan Amount to Purchase Price ratio")
```

## Scatterplots and barplots

```{r}
attach(Mortgagestats)
plot(Mortgagestats)
plot(Loan_Amount,Action_Taken, col = "#51CF44", ylab = "Approved(1), Denied(2)", 
     xlab = "Loan Amount(Thousands)", main = "Approved by Loan Amount")
plot(Race,Action_Taken, col = c("blue","#33B2FF"), ylab = "Approved(1), Denied(0)", 
     xlab = "White(1), Black(0)", main = "Approved by Race")
plot(Sex,Action_Taken, col = c("blue","#33B2FF"), ylab = "Approved(1), Denied(0)", 
     xlab = "Male(1), Female(0)", main = "Approved by Applicant's Sex")
plot(Income_Thousands,Action_Taken, col = "#51CF44", ylab = "Approved(1), Denied(2)", 
     xlab = "Income in Thousands", main = "Approved based on Income")
plot(Marital_Status,Action_Taken, col = c("blue","#33B2FF"), ylab = "Approved(1), Denied(0)", 
     xlab = "Married(M), Seperated(S), Unmarried(U)", main = "Approved by Marriage Status")
plot(Purchase_Price,Action_Taken, col = "#51CF44",  ylab = "Approved(1), Denied(2)",
     xlab = "Purchase Price(Thousands)", main = "Approved based on Purchase Price")
plot(DTI_Housing,Action_Taken, col = "#51CF44",  ylab = "Approved(1), Denied(2)",
     xlab = "Housing Debt to Income(DTI) Ratio", main = "Approved based on Housing DTI Ratio")
plot(DTI_Housing,Action_Taken, col = "#51CF44",  ylab = "Approved(1), Denied(2)",
     xlab = "Total Debt to Income(DTI) Ratio", main = "Approved based on Total DTI Ratio")
plot(School,Action_Taken, col = "#51CF44",  ylab = "Approved(1), Denied(2)",
     xlab = "Purchase Price(Thousands)", main = "Approved by Years of Education")
plot(Mortgage_Credit_History, Action_Taken,col = c("blue","#33B2FF"),
     ylab = "Approved(1), Denied(0)",
     xlab = "No Late Payments(1), No History(2), Late Payments (3,4)", 
     main = "Approved based on Mortgage Credit History")
```

## Composition of the Sample

- When examining our data we see that the average loan amount is for 139,000 with a large standard deviation, likely due to a few highly expensive homes in the data. The average applicant is a white male, a majority of applicants will be approved for their loans. The average income for applicants is around 76,000, though this has a wide spread as well due to the few incredibly high income applicants. Most applicants are not self-employed and most likely married. They have around an average of 16 years of schooling which means they completed college, although a lot of people fall at 12 years meaning they at least completed high school. We also see under mortgage credit history that most applicants have no mortgage history at all. The average applicant seems to have a housing debt to income ratio of .26, and a total debt to income ratio of .33. In other words their housing and total debts make up 26% and 33% of their income, respectively. This is a good sign that most applicants can take on more debt. We can also analyze the ratio of loan amount to purchase price and see that the average loan amount is 73% of the purchase price. 

## Data Transformation

- After looking over the spread of the data points we see that income and purchase price are skewed. We can perform a log transformation on those two columns to force the data into a more uniform distribution.(Unfortunately performing a log transformation puts near "inf" or infinite values for 0 when performing our models later on. Therefore it will not be used in the dataset. Otherwise it does fix the uniformity)
- we can also run a square root transformation and this will also help transform the uniformity of the two data points.


```{r}
hist(log(Mortgagestats$Income_Thousands), breaks = "fd", xlab = "Income(Thousands)", main = )
hist(log(Mortgagestats$Purchase_Price), breaks = "fd", xlab = "Purchase Price(Thousands)")
hist(sqrt(Mortgagestats$Income_Thousands), breaks = "fd", xlab = "Income(Thousands)")
hist(sqrt(Mortgagestats$Purchase_Price), breaks = "fd", xlab = "Purchase Price(Thousands)")
```

# Part C

C. With the full sample, estimate the logistic regression model, where the deny/approve dummy variable is the response variable and the debt-to-income ratio and the race, self-employed, marital status, and education indicator variables are the co-variates. Graph the ROC curve and calculate the AUC. Also, compute the confusion matrix at alternative cut-off levels, and calculate the classifier sensitivity, specificity, the false-positive rate, the false-negative rate, the model accuracy and error rate to confirm they are the same as those produced by R. Provide a written explanation summarizing the findings.

## Creating the logistical regression model

```{r}
MFit <- glm(Action_Taken ~ DTI_Housing + DTI_Total + Race + Self_Employed + Marital_Status + School, data = Mortgagestats, family = "binomial")

summary(MFit)
```

## ROC Curve, AUC and Threshold

- Here we create the ROC curve and calculate the AUC
- We see that our ROC best threshold is around .127 and our area under the curve is .726

```{r}
prob <- predict(MFit, Mortgagestats, type = "response")

my_roc <- roc(Mortgagestats$Action_Taken ~ prob, plot = TRUE, print.auc = TRUE)
coords(my_roc, "best", ret = "threshold")
```

## Confusion Matrices on Multiple Levels

```{r}
logit.pred <- factor(prob > 0.30, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))
logit.perf1 <- table(Mortgagestats$Action_Taken, logit.pred, dnn = c("Actual","Predicted"))
logit.perf1

logit.pred <- factor(prob > 0.5, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))
logit.perf2 <- table(Mortgagestats$Action_Taken, logit.pred, dnn = c("Actual","Predicted"))
logit.perf2

logit.pred <- factor(prob > 0.75, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))
logit.perf3 <- table(Mortgagestats$Action_Taken, logit.pred, dnn = c("Actual","Predicted"))
logit.perf3

logit.pred <- factor(prob > 0.1268, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))
logit.perf4 <- table(Mortgagestats$Action_Taken, logit.pred, dnn = c("Actual","Predicted"))
logit.perf4
```


## Checking Performance of models at multiple roc levels

```{r}
performance <- function(table, n = 4){
  if(!all(dim(table) == c(2,2)))
    stop("Must be a 2 x 2 table")
  tn = table[1,1]
  fp = table[1,2]
  fn = table[2,1]
  tp = table[2,2]
  sensitivity = tp/(tp+fn)
  specificity = tn/(tn+fp)
  ppp = tp/(tp+fp)
  npp = tn/(tn+fp)
  hitrate = (tp+tn)/(tp+tn+fp+fn)
  result <- paste0("Sensitivity = ", round(sensitivity,n),
                   "\nSpecificity = ",round(specificity,n),
                   "\nPositive Predictive Value = ", round(ppp,n),
                   "\nNegative Predictive Value = ", round(npp,n),
                   "\nAccuracy = ", round(hitrate,n))
  cat(result)
}
```

```{r}

performance(logit.perf1)
performance(logit.perf2)
performance(logit.perf3)
performance(logit.perf4)

```

- After reviewing our models we see that logit.perf 2 has the highest accuracy of .8858 with an ROC of 0.5. This model at 0.5 has near perfect specificity but almost no sensitivity. We calculate our optimal threshold ROC level at .1268, this would suggest that if we are 12.7% sure the predicted person is getting a mortgage that we should put them in the approved pile, the rest are denied. This is likely due to the fact that in the data the majority of the mortgages are approved. At this level our accuracy goes down to .7685. This sacrifices some accuracy and specificity to bring the sensitivity back to a more significant level. Bringing down the number that we accurately predict are approved but also bringing up the number that we accurately predict are not approved. 


# Part D

D. Next, using 10-fold cross validation, estimate a variety of logistic regression models and evaluate their predictive performance across a range of threshold values in each case. The models can (should) include interaction variables and polynomial terms (e.g., quadratic and cubic variables). Of interest is identifying the model and threshold value that yield the smallest average test misclassification rate; however, you can also calculate model accuracy and the AUC. Document in a table the performance of the various models using the chosen performance measures.


## Logistical Regression Models

```{r}
set.seed(123)
reg1 <- glm(Action_Taken ~ DTI_Housing + DTI_Total + Race + Self_Employed + Marital_Status + 
            School, data = Mortgagestats, family = "binomial")
cv.reg1 <- cv.glm(Mortgagestats, reg1, K = 10)$delta[2]

reg2 <- glm(Action_Taken ~ Loan_Amount + Race + Sex + Income_Thousands + Marital_Status + 
            Self_Employed + Purchase_Price + Mortgage_Credit_History + DTI_Housing + 
            DTI_Total + School, data = Mortgagestats, family = "binomial")
cv.reg2 <- cv.glm(Mortgagestats, reg2, K = 10)$delta[2]

reg3 <- glm(Action_Taken ~ (Loan_Amount/Purchase_Price) + Race + Sex + Income_Thousands + 
            Marital_Status + Self_Employed + Mortgage_Credit_History + DTI_Housing +
            DTI_Total + School, data = Mortgagestats, family = "binomial")
cv.reg3 <- cv.glm(Mortgagestats, reg3, K = 10)$delta[2]

reg4 <- step(reg3)
cv.reg4 <- cv.glm(Mortgagestats, reg4, K = 10)$delta[2]

reg5 <- glm(Action_Taken ~ (Loan_Amount/Purchase_Price) + Race + Sex + Income_Thousands + 
            Marital_Status + Self_Employed + Mortgage_Credit_History + 
            DTI_Housing + DTI_Total + School + (Income_Thousands)^2, 
            data = Mortgagestats, family = "binomial")
cv.reg5 <- cv.glm(Mortgagestats, reg5, K = 10)$delta[2]

reg6 <- step(reg5)
cv.reg6 <- cv.glm(Mortgagestats, reg6, K = 10)$delta[2]

cv.reg1
cv.reg2
cv.reg3
cv.reg4
cv.reg5
cv.reg6
```
### Predictions and ROC values of GLM models

- For this section I picked the three models with the lowest MSE values. This would be reg 2 with an MSE of .0934, reg 5 with an MSE of .0939, and reg 6 with an MSE of .0936. Reg 2 is our model with all chosen variable included, reg 5 uses the interaction variable between loan amount and purchase price along with making the income a quadratic variable. Finally reg 6 is a stepwise regression of reg 5. 

```{r}
prob1 <- predict(reg2, Mortgagestats, type = "response")
Reg1_Roc <- roc(Mortgagestats$Action_Taken ~ prob1)
coords(Reg1_Roc, "best", ret = "threshold")
reg.pred <- factor(prob1 > .136, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))
reg.perf1 <- table(Mortgagestats$Action_Taken, reg.pred, dnn = c("Actual","Predicted"))

prob2 <- predict(reg5, Mortgagestats, type = "response")
Reg2_Roc <- roc(Mortgagestats$Action_Taken ~ prob2)
coords(Reg2_Roc, "best", ret = "threshold")
reg.pred <- factor(prob2 > .121, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))
reg.perf2 <- table(Mortgagestats$Action_Taken, reg.pred, dnn = c("Actual","Predicted"))
reg.perf2

prob3 <- predict(reg6, Mortgagestats, type = "response")
Reg3_Roc <- roc(Mortgagestats$Action_Taken ~ prob3)
coords(Reg3_Roc, "best", ret = "threshold")
reg.pred <- factor(prob3 > .118, levels = c(FALSE,TRUE),
                     labels = c("Approved","Denied"))
reg.perf3 <- table(Mortgagestats$Action_Taken, reg.pred, dnn = c("Actual","Predicted"))
reg.perf3

reg.perf1
reg.perf2
reg.perf3
```


## Random Forest Model

```{r}
set.seed(1234)
reg.forest <- randomForest(Action_Taken ~ (Loan_Amount/Purchase_Price) + Race + Sex + 
            Marital_Status + Self_Employed + Mortgage_Credit_History + 
            DTI_Housing + DTI_Total + School, data=Mortgagestats, importance = TRUE)
reg.forest
importance(reg.forest, type = 2)

forest.pred <- predict(reg.forest, Mortgagestats, type = "response")
forest.perf <- table(Mortgagestats$Action_Taken, forest.pred, dnn = c("Actual","Predicted"))
forest.perf
```


## Support Vector Models

```{r}
set.seed(1234)
reg.svm <- svm(Action_Taken ~ (Loan_Amount/Purchase_Price) + Race + Sex + 
            Marital_Status + Self_Employed + Mortgage_Credit_History + 
            DTI_Housing + DTI_Total + School, data=Mortgagestats)

svm.pred <- predict(reg.svm, Mortgagestats)
svm.perf <- table(Mortgagestats$Action_Taken, svm.pred, dnn = c("Actual","Predicted"))
svm.perf
```

## Performance of Models

```{r}
performance(reg.perf1)
performance(reg.perf2)
performance(reg.perf3)
performance(forest.perf)
performance(svm.perf)
```

# Part E

E. Of the competing models that you estimated and thresholds that you evaluated, identify the superior model for classification purposes. Re-estimate the model with the full sample of data. Then, graph the ROC, calculate the AUC, and compute the confusion matrix at the threshold level associated with the minimum average test mis-classification rate . Calculate the classifier sensitivity and specificity, the false-positive rate, the false negative rate, the accuracy, and overall mis-classification rate. How well does your superior model perform relative to the model estimated in C? Explain. Note that to do so you will need to calculate the confusion matrix from the estimated model in C at the threshold level.

## Superior Model

```{r}
set.seed(1234)
reg.forest <- randomForest(Action_Taken ~ (Loan_Amount/Purchase_Price) + Race + Sex + 
            Marital_Status + Self_Employed + Mortgage_Credit_History + 
            DTI_Housing + DTI_Total + School, data=Mortgagestats, importance = TRUE)
reg.forest
```

- The random forest decision tree model is chosen because it has the highest accuracy in its confusion matrix. It can probably identify 99.8% of the data as shown in the area under curve in the ROC. The optimal threshold being at 1.5 does seem off though. This may be cause for revaluation. Although the performance of this model is shown to have perfect specificity and near perferct sensitivy. 

## ROC and AUC

```{r}
final.prob <- as.numeric(predict(reg.forest, Mortgagestats, type = "response"))
final_roc <- roc(Mortgagestats$Action_Taken ~ final.prob, plot = TRUE, print.auc = TRUE)
coords(final_roc, "best", ret = "threshold")


forest.pred <- predict(reg.forest, Mortgagestats, type = "response")
forest.perf <- table(Mortgagestats$Action_Taken, forest.pred, dnn = c("Actual","Predicted"))
forest.perf
```

## Confusion Matrix

```{r}
forest.perf
```


## Performance

```{r}
performance(forest.perf)
```

